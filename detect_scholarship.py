import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«
INPUT_CSV = "output/åŒ—æµ·é“_school_info.csv"
OUTPUT_CSV = "hokkaido_special_scholarship_flag.csv"

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
keywords = ["ç‰¹å¾…ç”Ÿ", "ç‰¹å¾…åˆ¶åº¦", "æˆæ¥­æ–™å…é™¤", "å­¦è²»å…é™¤", "å…¥å­¦é‡‘å…é™¤"]

# æœ€å¤§ã‚µãƒ–ãƒšãƒ¼ã‚¸æ•°
MAX_SUBPAGES = 5

# èª­ã¿è¾¼ã¿
df = pd.read_csv(INPUT_CSV)

results = []

for index, row in df.iterrows():
    school_name = row["é«˜æ ¡å"]
    base_url = row["å­¦æ ¡ç‹¬è‡ªURL"]

    if pd.isna(base_url) or not base_url.startswith("http"):
        continue

    print(f"ğŸ” å‡¦ç†ä¸­: {school_name} - {base_url}")
    found = False
    visited = set()

    try:
        res = requests.get(base_url, timeout=10)
        if res.status_code != 200:
            print(f"âŒ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—: {res.status_code}")
            continue
        soup = BeautifulSoup(res.content, "html.parser")
        text = soup.get_text()
        if any(k in text for k in keywords):
            found = True
        visited.add(base_url)

        if not found:
            links = soup.find_all("a", href=True)
            subpages = {urljoin(base_url, a["href"]) for a in links}
            count = 0
            for sub_url in subpages:
                if count >= MAX_SUBPAGES:
                    break
                if sub_url in visited:
                    continue
                if not urlparse(sub_url).netloc.endswith(urlparse(base_url).netloc):
                    continue
                try:
                    res_sub = requests.get(sub_url, timeout=10)
                    if res_sub.status_code != 200:
                        continue
                    soup_sub = BeautifulSoup(res_sub.content, "html.parser")
                    sub_text = soup_sub.get_text()
                    if any(k in sub_text for k in keywords):
                        found = True
                        break
                    visited.add(sub_url)
                    count += 1
                    time.sleep(0.5)
                except Exception as e:
                    print(f"âš ï¸ ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

        if found:
            print(f"âœ… ç‰¹å¾…åˆ¶åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç™ºè¦‹: {school_name}")
            results.append((school_name, base_url))
        else:
            print(f"ğŸš« ç‰¹å¾…åˆ¶åº¦ãªã—: {school_name}")

    except Exception as e:
        print(f"âš ï¸ {school_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        continue

    time.sleep(0.5)

# ä¿å­˜
result_df = pd.DataFrame(results, columns=["é«˜æ ¡å", "å­¦æ ¡ç‹¬è‡ªURL"])
result_df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
print(f"ğŸ‰ å‡ºåŠ›å®Œäº†: {OUTPUT_CSV}")


