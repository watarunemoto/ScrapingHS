import csv
import time
from urllib.parse import urlparse, parse_qs
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# å…¨å›½ã® (big_area, area) å¯¾å¿œè¡¨
prefectures = [
    (1, 1),  (2, 2),  (2, 3),  (2, 4),  (2, 5),  (2, 6),  (2, 7)
]
'''
prefectures = [
    (1, 1),  (2, 2),  (2, 3),  (2, 4),  (2, 5),  (2, 6),  (2, 7),
    (3, 8),  (3, 9),  (3, 10), (3, 11), (3, 12), (3, 13), (3, 14),
    (4, 15), (4, 19), (4, 20), (4, 21), (4, 22), (4, 23), (4, 24),
    (5, 16), (5, 17), (5, 18),
    (6, 25), (6, 26), (6, 27), (6, 28), (6, 29), (6, 30),
    (7, 31), (7, 32), (7, 33), (7, 34), (7, 35),
    (8, 36), (8, 37), (8, 38), (8, 39),
    (9, 40), (9, 41), (9, 42), (9, 43), (9, 44), (9, 45), (9, 46), (9, 47)
]
'''
base_url = "https://school.js88.com/kodawari?type=22"

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
wait = WebDriverWait(driver, 10)

all_school_urls = set()

print("ğŸ“¥ é«˜æ ¡ä¸€è¦§URLã‚’åé›†ä¸­...")
for big_area, area in prefectures:
    page = 1
    while True:
        url = f"{base_url}&big_area={big_area}&area={area}&page={page}"
        print(url)
        print(f"ğŸ“¦ {area}ï¼ˆ{big_area}ï¼‰ãƒšãƒ¼ã‚¸ {page} ã‚’å–å¾—ä¸­...")  # â† ã“ã®è¡Œã‚’è¿½åŠ 
        driver.get(url)
        time.sleep(1)
        links = driver.find_elements(By.XPATH, '//a[contains(@href, "/scl_h/")]')
        new_links = set()
        for link in links:
            href = link.get_attribute("href")
            if href and "/scl_h/" in href and "kodawari" not in href:
                school_id = href.split("/scl_h/")[1].split("/")[0]
                full_url = f"https://school.js88.com/scl_h/{school_id}/"
                new_links.add(full_url)
        if not new_links:
            break
        all_school_urls.update(new_links)
        page += 1

print(f"âœ… é«˜æ ¡URLå–å¾—å®Œäº†ï¼ˆ{len(all_school_urls)}æ ¡ï¼‰")

# å„é«˜æ ¡ã®è©³ç´°æƒ…å ±å–å¾—
results = []
for idx, url in enumerate(sorted(all_school_urls), start=1):
    print(f"ğŸ” [{idx}/{len(all_school_urls)}] {url}")
    driver.get(url)
    time.sleep(0.5)

    school_id = url.rstrip("/").split("/")[-1]
    try:
        school_name = wait.until(EC.presence_of_element_located((By.TAG_NAME, "h1"))).text.strip()
    except:
        school_name = ""

    try:
        address_full = wait.until(EC.presence_of_element_located((By.TAG_NAME, "address"))).text.strip()
        if "ã€’" in address_full:
            zip_code = address_full.split()[0].replace("ã€’", "")
            address = address_full.replace(f"ã€’{zip_code}", "").strip()
        else:
            zip_code = ""
            address = address_full
    except:
        zip_code = ""
        address = ""

    try:
        homepage_raw = wait.until(
            EC.presence_of_element_located((By.XPATH, '//dt[text()="ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]/following-sibling::dd[1]/a'))
        ).get_attribute("href")
        parsed = urlparse(homepage_raw)
        query = parse_qs(parsed.query)
        homepage = query.get("scl_url", [homepage_raw])[0]
    except:
        homepage = ""

    results.append([school_id, school_name, url, zip_code, address, homepage])

# CSVå‡ºåŠ›
with open("school_info.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["ID", "å­¦æ ¡å", "JS88_URL", "éƒµä¾¿ç•ªå·", "ä½æ‰€", "é«˜æ ¡ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"])
    writer.writerows(results)

print(f"\nâœ… å®Œäº†ï¼š{len(results)} ä»¶ã‚’ school_info.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

driver.quit()
