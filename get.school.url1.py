import time
import requests
from bs4 import BeautifulSoup
import pandas as pd

prefecture_table = [
    (1, 1, "åŒ—æµ·é“"),
    (2, 2, "é’æ£®çœŒ"), (2, 3, "å²©æ‰‹çœŒ"), (2, 4, "å®®åŸçœŒ"), (2, 5, "ç§‹ç”°çœŒ"), (2, 6, "å±±å½¢çœŒ"), (2, 7, "ç¦å³¶çœŒ"),
    (3, 8, "èŒ¨åŸçœŒ"), (3, 9, "æ ƒæœ¨çœŒ"), (3, 10, "ç¾¤é¦¬çœŒ"), (3, 11, "åŸ¼ç‰çœŒ"), (3, 12, "åƒè‘‰çœŒ"), (3, 13, "æ±äº¬éƒ½"), (3, 14, "ç¥å¥ˆå·çœŒ"),
    (4, 15, "æ–°æ½ŸçœŒ"), (4, 19, "å±±æ¢¨çœŒ"), (4, 20, "é•·é‡çœŒ"),
    (5, 21, "å²é˜œçœŒ"), (5, 22, "é™å²¡çœŒ"), (5, 23, "æ„›çŸ¥çœŒ"), (5, 24, "ä¸‰é‡çœŒ"),
    (6, 16, "å¯Œå±±çœŒ"),(6, 17, "çŸ³å·çœŒ"), (6, 18, "ç¦äº•çœŒ"),
    (7, 25, "æ»‹è³€çœŒ"), (7, 26, "äº¬éƒ½åºœ"), (7, 27, "å¤§é˜ªåºœ"), (7, 28, "å…µåº«çœŒ"), (7, 29, "å¥ˆè‰¯çœŒ"), (7, 30, "å’Œæ­Œå±±çœŒ"),
    (8, 31, "é³¥å–çœŒ"), (8, 32, "å³¶æ ¹çœŒ"), (8, 33, "å²¡å±±çœŒ"), (8, 34, "åºƒå³¶çœŒ"), (8, 35, "å±±å£çœŒ"), (8, 36, "å¾³å³¶çœŒ"), (8, 37, "é¦™å·çœŒ"), (8, 38, "æ„›åª›çœŒ"), (8, 39, "é«˜çŸ¥çœŒ"),
    (9, 40, "ç¦å²¡çœŒ"), (9, 41, "ä½è³€çœŒ"), (9, 42, "é•·å´çœŒ"), (9, 43, "ç†Šæœ¬çœŒ"), (9, 44, "å¤§åˆ†çœŒ"), (9, 45, "å®®å´çœŒ"), (9, 46, "é¹¿å…å³¶çœŒ"), (9, 47, "æ²–ç¸„çœŒ")
]

school_data = []

for big_area, area, pref_name in prefecture_table:
    page = 1
    while True:
        url = f"https://school.js88.com/kodawari?big-area={big_area}&area={area}&type=22&s={page}"
        res = requests.get(url)
        if res.status_code != 200:
            break
        soup = BeautifulSoup(res.content, "html.parser")
        school_blocks = soup.select("div.scl_name > h2 > a")
        if not school_blocks:
            break
        for a in school_blocks:
            school_name = a.text.strip()
            href = a.get("href")
            full_url = f"https://school.js88.com{href}"
            school_data.append((pref_name, school_name, full_url))
        print(f"âœ… {pref_name} ãƒšãƒ¼ã‚¸ {page}ï¼š{len(school_blocks)} ä»¶")
        page += 1
        time.sleep(0.5)

df = pd.DataFrame(school_data, columns=["éƒ½é“åºœçœŒ", "é«˜æ ¡å", "é«˜æ ¡ãƒšãƒ¼ã‚¸URL"])
df.to_csv("all_pref_schools.csv", index=False, encoding="utf-8-sig")
print("ğŸ‰ å®Œäº†ï¼šall_pref_schools.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")



'''
import time
import requests
from bs4 import BeautifulSoup
import pandas as pd

big_area = 3
area = 12
pref_name = "åƒè‘‰çœŒ"

school_data = []
page = 1

while True:
    url = f"https://school.js88.com/kodawari?big-area={big_area}&area={area}&type=22&s={page}"
    res = requests.get(url)
    if res.status_code != 200:
        break

    soup = BeautifulSoup(res.content, "html.parser")
    school_blocks = soup.select("div.scl_name > h2 > a")
    if not school_blocks:
        break

    for a in school_blocks:
        school_name = a.text.strip()
        href = a.get("href")
        full_url = f"https://school.js88.com{href}"
        school_data.append((pref_name, school_name, full_url))

    print(f"âœ… {pref_name} ãƒšãƒ¼ã‚¸ {page}ï¼š{len(school_blocks)} ä»¶")
    page += 1
    time.sleep(0.5)

df = pd.DataFrame(school_data, columns=["éƒ½é“åºœçœŒ", "é«˜æ ¡å", "é«˜æ ¡ãƒšãƒ¼ã‚¸URL"])
df.to_csv("chiba_schools.csv", index=False, encoding="utf-8-sig")
print("ğŸ‰ å®Œäº†ï¼šchiba_schools.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
'''
